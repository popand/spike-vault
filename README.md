# Volleyball Teams Data Aggregator

A robust data aggregation system that collects and processes information about volleyball teams from NCAA Division I, Division III, and Canadian universities. The system uses Temporal workflows for reliable task orchestration and features concurrent scraping capabilities.

## üåü Features

- **Multi-Source Data Collection**
  - NCAA Division I volleyball teams
  - NCAA Division III volleyball teams
  - Canadian university volleyball teams

- **Robust Workflow Orchestration**
  - Temporal-based workflow management
  - Concurrent scraping with rate limiting
  - Automatic retries and error handling
  - Configurable execution parameters

- **Data Processing**
  - Standardized data models
  - Automated data normalization
  - JSON-based storage with timestamps
  - Extensible storage backend

## üèó Architecture

### Workflow Structure
```
Main Aggregator Workflow
‚îú‚îÄ‚îÄ NCAA D1 Child Workflow
‚îÇ   ‚îî‚îÄ‚îÄ Scraping Activities
‚îú‚îÄ‚îÄ NCAA D3 Child Workflow
‚îÇ   ‚îî‚îÄ‚îÄ Scraping Activities
‚îî‚îÄ‚îÄ Canadian Universities Child Workflow
    ‚îî‚îÄ‚îÄ Scraping Activities
```

### Component Overview
- **Workflows**: Orchestrate the scraping process
  - `DataAggregatorWorkflow`: Main workflow coordinator
  - `ScrapeSourceWorkflow`: Individual source handler

- **Activities**: Perform actual work
  - `scrape_source`: Handles data collection
  - `store_results`: Manages data persistence

- **Models**: Define data structures
  - `Team`: Core team information
  - `Player`: Player details
  - `Coach`: Coaching staff information

## üöÄ Getting Started

### Prerequisites
- Python 3.8+
- Temporal server
- Git

### Installation

1. Clone the repository:
```bash
git clone https://github.com/popand/spike-vault.git
cd spike-vault
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Configure environment:
```bash
cp .env.example .env
# Edit .env with your settings
```

### Configuration

The system can be configured through environment variables:

```env
# Temporal Server Configuration
TEMPORAL_HOST=viaduct.proxy.rlwy.net
TEMPORAL_PORT=46280

# Scraping Configuration
SCRAPE_BATCH_SIZE=10
SCRAPE_DELAY_SECONDS=5

# Storage Configuration
OUTPUT_DIR=data
```

### Usage

1. Start the Temporal worker:
```bash
python -m volleyball_aggregator.worker
```

2. Run the aggregator workflow:
```bash
python -m volleyball_aggregator.run
```

## üìÅ Project Structure

```
volleyball_aggregator/
‚îú‚îÄ‚îÄ activities/
‚îÇ   ‚îî‚îÄ‚îÄ scraping.py      # Scraping activities
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ team.py          # Data models
‚îú‚îÄ‚îÄ scrapers/
‚îÇ   ‚îú‚îÄ‚îÄ base.py          # Base scraper class
‚îÇ   ‚îî‚îÄ‚îÄ ncaa.py          # NCAA implementation
‚îú‚îÄ‚îÄ workflows/
‚îÇ   ‚îî‚îÄ‚îÄ aggregator.py    # Workflow definitions
‚îú‚îÄ‚îÄ config.py            # Configuration management
‚îú‚îÄ‚îÄ worker.py            # Temporal worker
‚îî‚îÄ‚îÄ run.py              # Entry point
```

## üîß Development

### Adding New Data Sources

1. Create a new scraper class:
```python
from .base import BaseScraper

class NewSourceScraper(BaseScraper):
    async def get_team_list(self) -> List[str]:
        # Implement team list retrieval
        pass

    async def scrape_team(self, team_url: str) -> Team:
        # Implement team data scraping
        pass
```

2. Register the scraper in `activities/scraping.py`:
```python
scrapers = {
    "NEW_SOURCE": NewSourceScraper,
    # ... existing scrapers
}
```

### Error Handling

The system implements multiple layers of error handling:
- Activity-level retries with exponential backoff
- Workflow-level error recovery
- Logging for debugging and monitoring

## üìä Data Format

Example team data structure:
```json
{
  "school_name": "Example University",
  "division": "NCAA_D1",
  "conference": "Example Conference",
  "mascot": "Eagles",
  "head_coach": {
    "name": "Jane Doe",
    "title": "Head Coach",
    "years_at_school": 5
  },
  "players": [
    {
      "name": "Player Name",
      "number": "10",
      "position": "Outside Hitter"
    }
  ]
}
```

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìù License

This project is licensed under the MIT License - see the LICENSE file for details.

## üôè Acknowledgments

- [Temporal](https://temporal.io/) for workflow orchestration
- [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) for web scraping
- [Pydantic](https://pydantic-docs.helpmanual.io/) for data validation 